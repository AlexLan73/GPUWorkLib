1. Проверь и исправь нашу равлизацию в modules/fft_maxims
соответствует ли она нами же заявленая сруктура в /Doc/DrvGPU
используется ли эта библиотека /DrvGPU я а реализации modules/fft_maxims не увидел кеширования комманд очередей планов если такого нет исправь используй все наработки из библиотеки. позови синьеров 

2. подумай и предложи переписать правильно файлы из директории /DrvGPU и из /DrvGPU/common  я планировал в common будут находится файлы типа констант ля всего проекта а он разросса и еще будет расти 
добавить срртветствующие директории с файлами inteface, include, src может еще что то. давай сделаем правильно

3. Подумай и предложи решение сделать обертку типа Singelton для логгера и для сбора и отражения данных по профилированию потом может добавим запись данных в db, и вывод на кансоль в одном месте если 8 карт будут сразу выводить будет не правильно. Планирую много GPU много модулей и что бы в каждом не плодить вывод в одном месте.
 в шаблоне многопоточность, реализована очередь на входе, паттерн наблюдатель пришли данны прогамма проснулась считала из буфера и отработала. 
  3.1. logger
  3.2. Проффелирование GPU 
  3.3. Вывод на консоль
  3.4. db в будущем
  кто вызывает тот передает id GPU 
  сейчас в logger есть такая запись
   * Если path пустой, логи будут создаваться в:
     *   ./Logs/DRVGPU/YYYY-MM-DD/HH-MM-SS.log
     * 
     * Если path указан, логи будут создаваться в:
     *   ${path}/Logs/DRVGPU/YYYY-MM-DD/HH-MM-SS.log
так как у нас много GPU и не все сразу работают нужно в /DRVGPU/ добавить номер id должно быть так
${path}/Logs/DRVGPU_00/YYYY-MM-DD/HH-MM-SS.log
${path}/Logs/DRVGPU_01/YYYY-MM-DD/HH-MM-SS.log
${path}/Logs/DRVGPU_13/YYYY-MM-DD/HH-MM-SS.log

4. создать файл конфигурации configGPU.json для всего DrvGPU
 в нем сделать список рабочих GPU и параметры (параметры будут меняться)
 {
  [
    {
      id:1,
      nane:"Evgeni"
    }
  ],
  [
      id:0,
      nane:"Alex",
      is_prof:true,
      is_logger:true
  ]
 }
 
id - указывает к какой GPU что делать если не тпараметров к примеру is_.. это всегда false 
если при сарте нет configGPU.json его создать и записать 
      id:0, nane:"TEST", is_prof:true, is_logger:true

5. нужно доделать библиотеку modules/fft_maxima/ если у нас на зватает памяти  нужно разбить на batch и считать по блокам посмотри сдеть 
 \Doc\Modules\search_maxim\search_3_module-part4.cpp 
 и посмотри здесь  antenna_fft_core может уже реализованно  реализуй в концепции первого пункта используя  библиотеку /DrvGPU 
 посмотри как сделано в antenna_fft_release
уже есть специальный отлаженный kernal здесь прочитай fft_kernel_sources.hpp  Нужно про анализировать доступную память взть от ее 70% и под этот размер разить batch по антеннам (лучам) если в конце останется несколько 1..3 то добавить их в последний batch
и у нас с батчами по данныи будт часто возникать сложности может это вынести в главный DrvDPU что бы в других модулях не повторять.
И в этот модуль мы еще добавим функции поиска поэтому критически про анализируй antenna_fft_core на предмет расширения классов и гибкости. 

6. создай тестовый пример и помести его в fft_maxima/tests/ c расширением hpp и вызовом из гравного main как test_fft_maxima.hpp
данные формируются на CPU и через механизм SVM загружаем/выгружаем данные
пример в DrvGPU\tests\single_gpu.hpp

7. Добавь туда же тест если данные сформированны в дугой OpenCl и мы должны посчитать. то нам передают контехт этой карты и адрес данных на GPU через  cl_mem or clBuffer с выгрузкой результата на CPU и вcl_mem or clBuffer.

это будет большой многолетний проект 

собрать запустть на отладку. потом по шагам я сам проверю.
Используй sequential-thinking (MCP server) .
Шаг за шагом:
1. Прочитай все файлы
2. Пойми архитектуру
3. ПОТОМ пиши код
